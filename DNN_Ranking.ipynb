{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSLR-WEB10K\n",
      "Ranklib\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import collections\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pre-requisites and parameters for neural network\n",
    "import tensorflow as tf\n",
    "n_classes = 5\n",
    "n_data_dim = 136\n",
    "n_nodes_layer_1 = 512\n",
    "n_nodes_layer_2 = 512\n",
    "n_nodes_layer_3 = 256\n",
    "x = tf.placeholder(\"float\",[None,n_data_dim])\n",
    "y = tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct the NN\n",
    "def neural_network(x):\n",
    "    Weight = {'W_Hidden1':tf.Variable(tf.random_normal([n_data_dim,n_nodes_layer_1])),\n",
    "             'W_Hidden2':tf.Variable(tf.random_normal([n_nodes_layer_1,n_nodes_layer_2])),\n",
    "             'W_Hidden3':tf.Variable(tf.random_normal([n_nodes_layer_2,n_nodes_layer_3])),\n",
    "             'W_out':tf.Variable(tf.random_normal([n_nodes_layer_3,n_classes]))}\n",
    "    Bias = {'b_Hidden1':tf.Variable(tf.random_normal([n_nodes_layer_1])),\n",
    "            'b_Hidden2':tf.Variable(tf.random_normal([n_nodes_layer_2])),\n",
    "            'b_Hidden3':tf.Variable(tf.random_normal([n_nodes_layer_3])),\n",
    "            'b_out':tf.Variable(tf.random_normal([n_classes]))\n",
    "           }\n",
    "    \n",
    "    #First hidden layer\n",
    "    hidden_layer1_input = tf.add(tf.matmul(x,Weight['W_Hidden1']),Bias['b_Hidden1'])\n",
    "    hidden_layer1_output = tf.nn.relu(hidden_layer1_input)\n",
    "    \n",
    "    #Second hidden layer\n",
    "    hidden_layer2_input = tf.add(tf.matmul(hidden_layer1_output,Weight['W_Hidden2']),Bias['b_Hidden2'])\n",
    "    hidden_layer2_ouput = tf.nn.relu(hidden_layer2_input)\n",
    "    \n",
    "    #Third hidden layer\n",
    "    hidden_layer3_input = tf.add(tf.matmul(hidden_layer2_ouput,Weight['W_Hidden3']),Bias['b_Hidden3'])\n",
    "    hidden_layer3_output = tf.nn.relu(hidden_layer3_input)\n",
    "    \n",
    "    #output layer\n",
    "    output = tf.add(tf.matmul(hidden_layer3_output,Weight['W_out']),Bias['b_out'])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_txt(foldDir,foldsize,foldnum):\n",
    "    corpusDir = os.path.join('%s%s' % (inputDir,  os.listdir(foldDir)[foldsize]))\n",
    "    dataDir = os.path.join('%s/%s' % (corpusDir, os.listdir(corpusDir)[foldnum]))\n",
    "    trainDir = os.path.join('%s/%s' % (dataDir, \"train.txt\"))\n",
    "    testDir = os.path.join('%s/%s' % (dataDir, \"test.txt\"))\n",
    "    validDir = os.path.join('%s/%s' % (dataDir, \"vali.txt\"))\n",
    "    return [trainDir,testDir ,validDir]\n",
    "\n",
    "# Dropping column with 100% nan values (parsing problem probably)\n",
    "def drop_col(df):\n",
    "    df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "\n",
    "# Cleaning values\n",
    "def split_semicolon(df):\n",
    "    # removes string naming pattern '*:' from values\n",
    "    for col in range(1,len(df.columns)):\n",
    "        df.loc[:,col] = df.loc[:,col].apply(lambda x: str(x).split(':')[1])\n",
    "    df.columns = ['rel', 'qid'] + [str(x) for x in range(1,137)] # renaming cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_csv(foldDir,foldsize,foldnum):\n",
    "    corpusDir = os.path.join('%s%s' % (inputDir,  os.listdir(foldDir)[foldsize]))\n",
    "    dataDir = os.path.join('%s/%s' % (corpusDir, os.listdir(corpusDir)[foldsize]))\n",
    "    trainDir = os.path.join('%s/%s' % (dataDir, \"train.csv\"))\n",
    "    testDir = os.path.join('%s/%s' % (dataDir, \"test.csv\"))\n",
    "    validDir = os.path.join('%s/%s' % (dataDir, \"vali.csv\"))\n",
    "    return [trainDir,testDir ,validDir]\n",
    "\n",
    "inputDir = \"../input/\"\n",
    "dataDir = load_csv(inputDir,1,1)\n",
    "\n",
    "trainset = pd.read_csv(dataDir[0])\n",
    "trainset.drop(trainset.columns[0], axis=1, inplace=True)\n",
    "testset  = pd.read_csv(dataDir[1])\n",
    "testset.drop(testset.columns[0], axis=1, inplace=True)\n",
    "validset = pd.read_csv(dataDir[2])\n",
    "validset.drop(validset.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normarlization():\n",
    "    def __init__(self,train_input,valid_input,test_input,concate_normalize=False):\n",
    "        if concate_normalize == True:\n",
    "            whole_data = np.concatenate((train_input,valid_input,train_input),axis=0)\n",
    "        else:\n",
    "            whole_data = train_input\n",
    "        self.normalize_factor = np.max(whole_data,axis=0)\n",
    "        self.normalize_factor[self.normalize_factor==0] = 1\n",
    "            \n",
    "    def perform_normalize(self,input_data):\n",
    "        return np.divide(input_data,self.normalize_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy import inf\n",
    "\n",
    "def NDCG_metric(dataset,predict,target,position):\n",
    "    Result_predict = collections.defaultdict(int)\n",
    "    Result_target = collections.defaultdict(int)\n",
    "    Result_normalization = collections.defaultdict(int)\n",
    "\n",
    "    index = 0\n",
    "    \n",
    "    qid = qid_old = dataset['qid'][0]\n",
    "    for i in range(len(dataset)): \n",
    "        qid = dataset['qid'][i]\n",
    "        \n",
    "        if qid != qid_old:\n",
    "            index = 0\n",
    "        \n",
    "        if index < position:\n",
    "            Result_predict[qid] += (np.power(2, predict[i])-1)/(np.log2(index+2))\n",
    "            Result_target[qid] += (np.power(2,  target[i])-1)/(np.log2(index+2))\n",
    "            index += 1\n",
    "            qid_old = qid\n",
    "       \n",
    "    for key in Result_predict.keys():\n",
    "        Result_normalization[key] = Result_predict[key]/Result_target[key] if Result_target[key] > 0 else 1\n",
    "     \n",
    "    data = [Result_predict.values(),Result_target.values(),Result_normalization.values()]\n",
    "    df = pd.DataFrame(data,index=['predict','target','NDCG'],columns=Result_predict.keys())\n",
    "    return df.T\n",
    "\n",
    "\n",
    "def sigmoid_f(z):\n",
    "    z = -z\n",
    "    z = np.exp(z)\n",
    "    z = 1+z\n",
    "    z = 1/z\n",
    "    return z\n",
    "\n",
    "def softmax(x):\n",
    "#     print(np.exp(x))\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=1)).T\n",
    "\n",
    "\n",
    "def neural_network_ranking(X_train, y_train, X_validation=None, y_validation=None, X_test=None, y_test=None, accuracy_period=1, model_path=\"../model/logistic_w.npy\", learning_rate=0.00001, batch_size=64, number_of_runs=1000, lambda_v=0):\n",
    "    nomarlizer = Normarlization(train_input=X_train, valid_input=X_valid, test_input=X_test,concate_normalize=False)\n",
    "    print(\"normalizing train\")\n",
    "    X_train = nomarlizer.perform_normalize(X_train)\n",
    "#     X_train = np.append(X_train, np.ones((X_train.shape[0], 1)), axis=1)\n",
    "    n_data = X_train.shape[0]\n",
    "    print(X_train.shape,y_train.shape)\n",
    "#     w = np.random.rand(X_train.shape[1], y_train.shape[1]) * 2 - 1\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "#     try:\n",
    "#         np.save(model_path, w)\n",
    "#     except Exception as e:\n",
    "#         print(\"The save path doesn't exist, creating a new one...\")\n",
    "# #         print(e)\n",
    "# #         import sys\n",
    "# #         sys.exit()\n",
    "#         os.makedirs(model_path)\n",
    "\n",
    "    m = batch_size\n",
    "    test_accuracy = \"\"\n",
    "    validation_accuracy = \"\"\n",
    "    \n",
    "    test_flag = 0\n",
    "    if type(X_test) != type(None) and type(y_test) != type(None):\n",
    "        print(\"normalizing test\")\n",
    "        test_flag = 1\n",
    "        X_test = nomarlizer.perform_normalize(X_test)\n",
    "#         X_test = np.append(X_test, np.ones((X_test.shape[0], 1)), axis=1)\n",
    "\n",
    "\n",
    "    validation_flag = 0\n",
    "    if type(X_validation) != type(None) and type(y_validation) != type(None):\n",
    "        print(\"normalizing validation\")\n",
    "        validation_flag = 1\n",
    "        X_validation = nomarlizer.perform_normalize(X_validation)\n",
    "#         X_validation = np.append(X_validation, np.ones((X_validation.shape[0], 1)), axis=1)\n",
    "\n",
    "    #Tensors for the neural network\n",
    "    prediction = neural_network(x)\n",
    "    prob_prediction = tf.nn.softmax(prediction)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "    optimiser = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"start training\")\n",
    "        prev_ndcg = 0\n",
    "        test_result_accuracy = 0\n",
    "        for i in range(number_of_runs):\n",
    "            current_Epoch_Loss = 0\n",
    "            random_list = random.sample(range(n_data), n_data)\n",
    "            for j in range(n_data//batch_size):\n",
    "                x_batch = X_train[random_list[j*batch_size:(j+1)*batch_size]]\n",
    "                y_batch = y_train[random_list[j*batch_size:(j+1)*batch_size]]\n",
    "                _,currentloss = sess.run([optimiser,loss],feed_dict = {x:x_batch,y:y_batch})\n",
    "                current_Epoch_Loss += currentloss\n",
    "\n",
    "            if (i+1) % accuracy_period == 0:\n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prob_prediction,axis=1),tf.argmax(y,axis=1)),'float'))\n",
    "                train_dict = {x:X_train,y:y_train}\n",
    "                train_accuracy = accuracy.eval(train_dict)\n",
    "\n",
    "                if test_flag:\n",
    "                    test_dict = {x:X_test,y:y_test}\n",
    "                    test_accuracy = accuracy.eval(test_dict)\n",
    "                    y_test_pre_value = np.argmax(prob_prediction.eval(test_dict), axis=1)\n",
    "                    y_test_value = np.argmax(y_test, axis=1)\n",
    "                    NDCG_test = NDCG_metric(testset,y_test_pre_value,y_test_value,5)\n",
    "                    score = np.mean(NDCG_test['NDCG'])\n",
    "                    print ('The NDCG socre on test set : '  + str(score))\n",
    "                    print(y_test_pre_value[0:30])\n",
    "                    print(y_test_value[0:30])\n",
    "\n",
    "\n",
    "                if validation_flag:\n",
    "                    validation_dict = {x:X_validation,y:y_validation}\n",
    "                    validation_accuracy = accuracy.eval(validation_dict)\n",
    "                    y_validation_pre_value = np.argmax(prob_prediction.eval(validation_dict), axis=1)\n",
    "                    y_validation_value = np.argmax(y_validation, axis=1)\n",
    "    #             print(np.min(np.min(np.log(y_pre),axis=1)))\n",
    "    #             print(np.min(np.min(np.log(np.subtract(1,y_pre)),axis=1)))\n",
    "#                 y_pre_log_Complement = np.log(1 - y_pre)\n",
    "#                 y_pre_log_Complement[y_pre_log_Complement==-inf] = -5000\n",
    "#                 cost = -1 / m * sum(sum((y_batch * np.log(y_pre) + (1 - y_batch) * y_pre_log_Complement)))\n",
    "                print(str(i).ljust(10) + str(train_accuracy).ljust(30) + str(validation_accuracy).ljust(30)+str(test_accuracy).ljust(30) + str(current_Epoch_Loss))\n",
    "                \n",
    "                if score>0.2 and test_accuracy>0.4 and score<0.92:\n",
    "                    if score>prev_ndcg:\n",
    "                        saver = tf.train.Saver()\n",
    "                        saver.save(sess, model_path+'DNN.checkpoint')\n",
    "#                         np.save(model_path, w)\n",
    "                        prev_ndcg = score\n",
    "                        test_result_accuracy = test_accuracy\n",
    "                        \n",
    "#     np.save(model_path, w)\n",
    "    print(\"model saved to \" + model_path)\n",
    "    print('The highest Normalized Discounted Culmulative Gain is:',prev_ndcg,'and the corresponding accuracy is:',test_result_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_sort = trainset.sort_values(by=['qid','rel'],ascending=False)\n",
    "validset_sort = validset.sort_values(by=['qid','rel'],ascending=False)\n",
    "testset_sort = testset.sort_values(by=['qid','rel'],ascending=False)\n",
    "\n",
    "y_train = trainset_sort['rel'].values\n",
    "X_train = trainset_sort.drop(['rel','qid'],axis=1).values\n",
    "\n",
    "y_valid = validset_sort['rel'].values\n",
    "X_valid = validset_sort.drop(['rel','qid'],axis=1).values\n",
    "\n",
    "y_test = testset_sort['rel'].values\n",
    "X_test = testset_sort.drop(['rel','qid'],axis=1).values\n",
    "\n",
    "y_train_one_hot = np.zeros((y_train.shape[0],5))\n",
    "y_train_one_hot[np.arange(y_train.shape[0]), y_train] = 1\n",
    "\n",
    "y_vali_one_hot = np.zeros((y_valid.shape[0],5))\n",
    "y_vali_one_hot[np.arange(y_valid.shape[0]), y_valid] = 1\n",
    "\n",
    "y_test_one_hot = np.zeros((y_test.shape[0],5))\n",
    "y_test_one_hot[np.arange(y_test.shape[0]), y_test] = 1\n",
    "\n",
    "# y_train_one_hot.shape,X_train.shape,y_test_one_hot.shape,X_test.shape\n",
    "# X_train = np.append(X_train, np.ones((X_train.shape[0],1)), axis=1)\n",
    "# n_data = X_train.shape[0]\n",
    "# X_validation = np.append(X_valid, np.ones((X_valid.shape[0],1)), axis=1)\n",
    "# X_test = np.append(X_test, np.ones((X_test.shape[0],1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723412, 136)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing train\n",
      "(723412, 136) (723412, 5)\n",
      "normalizing test\n",
      "normalizing validation\n",
      "start training\n",
      "The NDCG socre on test set : 0.904656140151\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0         0.321489                      0.322262                      0.322523                      1.43950315789e+14\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "1         0.522464                      0.516546                      0.516659                      4567.50007272\n",
      "The NDCG socre on test set : 1.75796842045\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "2         0.131435                      0.135638                      0.134394                      4575.00389171\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "3         0.522464                      0.516546                      0.516659                      4564.96271002\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "4         0.522464                      0.516546                      0.516659                      4513.31349456\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "5         0.522464                      0.516546                      0.516659                      4565.69783688\n",
      "The NDCG socre on test set : 0.904656140151\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "6         0.321489                      0.322262                      0.322523                      4567.16845131\n",
      "The NDCG socre on test set : 1.75796842045\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "7         0.131435                      0.135638                      0.134394                      4555.86106932\n",
      "The NDCG socre on test set : 0.904656140151\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "8         0.321489                      0.322262                      0.322523                      4560.01268852\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "9         0.522464                      0.516546                      0.516659                      4572.98874474\n",
      "The NDCG socre on test set : 0.904656140151\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "10        0.321489                      0.322262                      0.322523                      4574.360098\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "11        0.522464                      0.516546                      0.516659                      4555.00301945\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "12        0.522464                      0.516546                      0.516659                      4570.35639966\n",
      "The NDCG socre on test set : 1.75796842045\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "13        0.131435                      0.135638                      0.134394                      4578.73787141\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "14        0.522464                      0.516546                      0.516659                      4599.87674057\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "15        0.522464                      0.516546                      0.516659                      4540.34051979\n",
      "The NDCG socre on test set : 0.904656140151\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "16        0.321489                      0.322262                      0.322523                      4563.26039672\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "17        0.522464                      0.516546                      0.516659                      4543.61234844\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "18        0.522464                      0.516546                      0.516659                      4536.77679813\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "19        0.522464                      0.516546                      0.516659                      4583.57693088\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "20        0.522464                      0.516546                      0.516659                      4564.32417202\n",
      "The NDCG socre on test set : 3.46459298106\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "21        0.0174976                     0.0178909                     0.0184249                     4596.90446126\n",
      "The NDCG socre on test set : 1.75796842045\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "22        0.131435                      0.135638                      0.134394                      4565.71775365\n",
      "The NDCG socre on test set : 1.75796842045\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "23        0.131435                      0.135638                      0.134394                      4525.97179365\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "24        0.522464                      0.516546                      0.516659                      4565.82096279\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "25        0.522464                      0.516546                      0.516659                      4569.97744381\n",
      "The NDCG socre on test set : 1.75796842045\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "26        0.131435                      0.135638                      0.134394                      4587.845258\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "27        0.522464                      0.516546                      0.516659                      4527.01385736\n",
      "The NDCG socre on test set : 0.904656140151\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "28        0.321489                      0.322262                      0.322523                      4534.21019268\n",
      "The NDCG socre on test set : 0.478\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29        0.522464                      0.516546                      0.516659                      4582.26968467\n",
      "model saved to ./model_DNN_test/\n",
      "The highest Normalized Discounted Culmulative Gain is: 0.478 and the corresponding accuracy is: 0.516659\n"
     ]
    }
   ],
   "source": [
    "neural_network_ranking(batch_size=1024, number_of_runs=30, learning_rate=1e-5, accuracy_period=1,\n",
    "               model_path=\"./model_DNN_test/\", X_train=X_train, y_train=y_train_one_hot, X_validation=X_valid,\n",
    "               y_validation=y_vali_one_hot, X_test=X_test, y_test=y_test_one_hot, lambda_v=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
